#cloud-config
# AI Services GPU Inference Server (Docker-based)
# Services: Whisper (speech-to-text), Ollama (LLM inference)
# Requires: debian-12-nvidia template (NVIDIA drivers pre-installed)
# Sets up: Docker, NVIDIA Container Toolkit, Whisper API, Ollama, Watchtower

hostname: ai-gpu
manage_etc_hosts: true

package_update: true
package_upgrade: false

packages:
  - ca-certificates
  - curl
  - gnupg
  - git
  - wget
  - htop
  - unzip
  - nginx
  - certbot
  - python3-certbot-dns-cloudflare

users:
  - default
  - name: ai
    groups: [sudo, video, render, docker]
    shell: /bin/bash
    sudo: ALL=(ALL) NOPASSWD:ALL

write_files:
  # Cloudflare credentials for Let's Encrypt
  - path: /etc/letsencrypt/cloudflare.ini
    permissions: "0600"
    content: |
      dns_cloudflare_api_token = ${cloudflare_api_token}

  # Nginx configuration - routes /transcribe to Whisper, /generate and /api to Ollama
  - path: /etc/nginx/sites-available/ai
    permissions: "0644"
    content: |
      server {
          listen 80;
          server_name ${domain};

          location / {
              return 301 https://$host$request_uri;
          }
      }

      server {
          listen 443 ssl;
          server_name ${domain};

          ssl_certificate /etc/letsencrypt/live/${domain}/fullchain.pem;
          ssl_certificate_key /etc/letsencrypt/live/${domain}/privkey.pem;
          ssl_protocols TLSv1.2 TLSv1.3;
          ssl_ciphers HIGH:!aNULL:!MD5;

          client_max_body_size 100M;

          # Health check endpoint
          location = /health {
              proxy_pass http://127.0.0.1:8000/health;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
          }

          # Whisper transcription endpoints
          location /transcribe {
              proxy_pass http://127.0.0.1:8000/transcribe;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;

              # Extended timeouts for audio processing
              proxy_read_timeout 600s;
              proxy_connect_timeout 60s;
              proxy_send_timeout 600s;
              proxy_buffering off;
              proxy_request_buffering off;
          }

          # Ollama generate endpoint (for custom prompts)
          location /generate {
              proxy_pass http://127.0.0.1:11434/api/generate;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;

              # Extended timeouts for LLM processing
              proxy_read_timeout 300s;
              proxy_connect_timeout 60s;
              proxy_send_timeout 300s;
          }

          # Ollama chat endpoint (OpenAI-compatible)
          location /v1/chat/completions {
              proxy_pass http://127.0.0.1:11434/v1/chat/completions;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_read_timeout 300s;
          }

          # Ollama API (full access)
          location /api/ {
              proxy_pass http://127.0.0.1:11434/api/;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_read_timeout 300s;
          }

          # Queue status (whisper)
          location /queue {
              proxy_pass http://127.0.0.1:8000/queue;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
          }

          # Swagger docs for whisper
          location /docs {
              proxy_pass http://127.0.0.1:8000/docs;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
          }

          location /openapi.json {
              proxy_pass http://127.0.0.1:8000/openapi.json;
              proxy_set_header Host $host;
          }

          # Root shows service info
          location = / {
              default_type application/json;
              return 200 '{"service":"AI Services","endpoints":{"/transcribe":"Speech-to-text (Whisper)","/generate":"Text generation (Ollama)","/v1/chat/completions":"OpenAI-compatible chat","/api/":"Full Ollama API","/health":"Health check","/docs":"Whisper API docs"}}';
          }
      }

  # Cert renewal hook
  - path: /etc/letsencrypt/renewal-hooks/deploy/reload-nginx.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      systemctl reload nginx


  # Docker Compose configuration
  - path: /opt/ai/docker-compose.yaml
    permissions: "0644"
    content: |
      services:
        whisper-api:
          image: ghcr.io/${github_owner}/whisper-api:latest
          container_name: whisper-api
          restart: unless-stopped
          ports:
            - "127.0.0.1:8000:8000"
          volumes:
            - whisper-cache:/app/.cache
          environment:
            - WHISPER_MODEL=turbo
            - WHISPER_DEVICE=cuda
            - XDG_CACHE_HOME=/app/.cache
          deploy:
            resources:
              reservations:
                devices:
                  - driver: nvidia
                    count: 1
                    capabilities: [gpu]
          logging:
            driver: json-file
            options:
              max-size: "10m"
              max-file: "3"

        ollama:
          image: ollama/ollama:latest
          container_name: ollama
          restart: unless-stopped
          ports:
            - "127.0.0.1:11434:11434"
          volumes:
            - ollama-data:/root/.ollama
          environment:
            - OLLAMA_KEEP_ALIVE=5m
          deploy:
            resources:
              reservations:
                devices:
                  - driver: nvidia
                    count: 1
                    capabilities: [gpu]
          logging:
            driver: json-file
            options:
              max-size: "10m"
              max-file: "3"

        watchtower:
          image: nickfedor/watchtower
          container_name: watchtower
          restart: unless-stopped
          volumes:
            - /var/run/docker.sock:/var/run/docker.sock
            - /home/ai/.docker/config.json:/config.json:ro
          environment:
            - WATCHTOWER_CLEANUP=true
            - WATCHTOWER_POLL_INTERVAL=300
            - WATCHTOWER_INCLUDE_STOPPED=false
            - DOCKER_CONFIG=/
          command: whisper-api ollama

      volumes:
        whisper-cache:
        ollama-data:

  # Main setup script - runs once on first boot
  - path: /opt/ai/setup.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # AI Services Docker Setup Script
      # NVIDIA drivers are pre-installed in the template

      LOG_FILE="/var/log/ai-setup.log"
      exec > >(tee -a "$LOG_FILE") 2>&1

      echo ""
      echo "=========================================="
      echo "AI Services Setup - $(date)"
      echo "=========================================="

      SETUP_COMPLETE="/opt/ai/.setup_complete"

      # If setup is complete, exit
      if [ -f "$SETUP_COMPLETE" ]; then
          echo "Setup already complete."
          exit 0
      fi

      # Verify GPU is available
      echo ""
      echo "=== Checking GPU ==="
      if ! nvidia-smi; then
          echo "ERROR: NVIDIA GPU not detected!"
          echo "Make sure you're using the debian-12-nvidia template"
          echo "and GPU passthrough is configured correctly."
          exit 1
      fi

      echo ""
      echo "GPU detected:"
      nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv

      # Install NVIDIA Container Toolkit
      echo ""
      echo "=== Installing NVIDIA Container Toolkit ==="
      if ! command -v nvidia-ctk &> /dev/null; then
          curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
          curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
              sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
              tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
          apt-get update
          apt-get install -y nvidia-container-toolkit
          nvidia-ctk runtime configure --runtime=docker
          systemctl restart docker
          echo "NVIDIA Container Toolkit installed"
      else
          echo "NVIDIA Container Toolkit already installed"
      fi

      # Setup GHCR authentication
      echo ""
      echo "=== Configuring GHCR authentication ==="
      mkdir -p /home/ai/.docker
      echo '{"auths":{"ghcr.io":{"auth":"${ghcr_auth}"}}}' > /home/ai/.docker/config.json
      chown -R ai:ai /home/ai/.docker
      chmod 600 /home/ai/.docker/config.json
      echo "GHCR authentication configured"

      # Ensure ownership
      chown -R ai:ai /opt/ai

      # Pull and start containers
      echo ""
      echo "=== Starting AI Services containers ==="
      cd /opt/ai
      sudo -u ai docker compose pull
      sudo -u ai docker compose up -d

      # Wait for Ollama to be ready and pull the model
      echo ""
      echo "=== Pulling Ollama model (${ollama_model}) ==="
      for i in {1..30}; do
          if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
              break
          fi
          echo "Waiting for Ollama... ($i/30)"
          sleep 5
      done

      # Pull the translation model
      echo "Pulling ${ollama_model}..."
      curl -X POST http://localhost:11434/api/pull -d '{"name": "${ollama_model}"}' --no-buffer

      # Wait for Whisper API to be ready
      echo ""
      echo "=== Waiting for Whisper API to be ready ==="
      for i in {1..60}; do
          if curl -s http://localhost:8000/health | grep -q "healthy"; then
              break
          fi
          echo "Waiting... ($i/60)"
          sleep 5
      done

      # Verify setup
      WHISPER_OK=$(curl -s http://localhost:8000/health | grep -q "healthy" && echo "yes" || echo "no")
      OLLAMA_OK=$(curl -s http://localhost:11434/api/tags | grep -q "${ollama_model}" && echo "yes" || echo "no")

      if [ "$WHISPER_OK" = "yes" ] && [ "$OLLAMA_OK" = "yes" ]; then
          touch "$SETUP_COMPLETE"
          chown ai:ai "$SETUP_COMPLETE"

          echo ""
          echo "=========================================="
          echo "SETUP COMPLETE!"
          echo "=========================================="
          echo ""
          echo "AI Services: https://${domain}"
          echo ""
          echo "Endpoints:"
          echo "  - Transcription: POST https://${domain}/transcribe"
          echo "  - Generate:      POST https://${domain}/generate"
          echo "  - Chat:          POST https://${domain}/v1/chat/completions"
          echo "  - Health:        GET  https://${domain}/health"
          echo ""
          echo "Status:"
          echo "  - Whisper API: $WHISPER_OK"
          echo "  - Ollama:      $OLLAMA_OK"
          echo ""

          # Disable setup service
          systemctl disable ai-setup.service
      else
          echo "ERROR: Some services failed to start"
          echo "  - Whisper API: $WHISPER_OK"
          echo "  - Ollama: $OLLAMA_OK"
          docker compose logs
          exit 1
      fi

  # Systemd service for setup (runs once on first boot)
  - path: /etc/systemd/system/ai-setup.service
    permissions: "0644"
    content: |
      [Unit]
      Description=AI Services Docker Setup
      After=network-online.target docker.service
      Wants=network-online.target
      Requires=docker.service

      [Service]
      Type=oneshot
      ExecStart=/opt/ai/setup.sh
      RemainAfterExit=yes
      StandardOutput=journal+console
      StandardError=journal+console

      [Install]
      WantedBy=multi-user.target

  # Promtail configuration for shipping logs to Loki
  - path: /etc/promtail/config.yaml
    permissions: "0644"
    content: |
      server:
        http_listen_port: 9080
        grpc_listen_port: 0

      positions:
        filename: /var/lib/promtail/positions.yaml

      clients:
        - url: http://192.168.20.11:31100/loki/api/v1/push

      scrape_configs:
        # Scrape Docker container logs
        - job_name: docker
          docker_sd_configs:
            - host: unix:///var/run/docker.sock
              refresh_interval: 5s
          relabel_configs:
            - target_label: job
              replacement: docker
            - source_labels: ['__meta_docker_container_name']
              regex: '/(.+)'
              target_label: container
            - source_labels: ['__meta_docker_container_name']
              regex: '/(whisper-api|ollama|watchtower)'
              action: keep
            - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
              target_label: compose_service
            - source_labels: ['__meta_docker_container_label_com_docker_compose_project']
              target_label: compose_project
            - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
              target_label: service_name
            - source_labels: ['__meta_docker_container_id']
              target_label: __path__
              replacement: /var/lib/docker/containers/$1/$1-json.log
            - action: labelkeep
              regex: '^(job|container|compose_service|compose_project|service_name)$'
          pipeline_stages:
            - json:
                expressions:
                  log: log
                  stream: stream
                  time: time
            - output:
                source: log

        - job_name: system
          journal:
            max_age: 12h
            labels:
              job: system
              host: ai-gpu
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              regex: '(sshd|systemd-.*|nvidia-.*|docker.*)\.service'
              action: keep
            - source_labels: ['__journal__systemd_unit']
              target_label: unit
            - source_labels: ['__journal_priority_keyword']
              target_label: level

  # Systemd service for Promtail
  - path: /etc/systemd/system/promtail.service
    permissions: "0644"
    content: |
      [Unit]
      Description=Promtail Log Collector
      After=network-online.target docker.service
      Wants=network-online.target

      [Service]
      Type=simple
      User=root
      ExecStart=/usr/local/bin/promtail -config.file=/etc/promtail/config.yaml
      Restart=always
      RestartSec=10

      [Install]
      WantedBy=multi-user.target

  # Promtail installation script
  - path: /opt/ai/install-promtail.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      set -e

      PROMTAIL_VERSION="3.6.3"
      ARCH="amd64"

      echo "Installing Promtail $${PROMTAIL_VERSION}..."

      cd /tmp
      wget -q "https://github.com/grafana/loki/releases/download/v$${PROMTAIL_VERSION}/promtail-linux-$${ARCH}.zip"
      unzip -o "promtail-linux-$${ARCH}.zip"
      chmod +x "promtail-linux-$${ARCH}"
      mv "promtail-linux-$${ARCH}" /usr/local/bin/promtail
      rm -f "promtail-linux-$${ARCH}.zip"

      mkdir -p /var/lib/promtail
      mkdir -p /etc/promtail

      systemctl daemon-reload
      systemctl enable promtail
      systemctl start promtail

      echo "Promtail installed and started!"

runcmd:
  - mkdir -p /opt/ai
  - chown -R ai:ai /opt/ai
  - echo "AI services cloud-init started at $(date)" >> /var/log/ai-setup.log

  # Install Docker CE from official repository
  - install -m 0755 -d /etc/apt/keyrings
  - curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc
  - chmod a+r /etc/apt/keyrings/docker.asc
  - |
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian $(. /etc/os-release && echo $VERSION_CODENAME) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
  - apt-get update
  - apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  - usermod -aG docker ai
  - systemctl enable docker
  - systemctl start docker

  # Create letsencrypt directories
  - mkdir -p /etc/letsencrypt/renewal-hooks/deploy

  # Get Let's Encrypt certificate
  - |
    certbot certonly \
      --dns-cloudflare \
      --dns-cloudflare-credentials /etc/letsencrypt/cloudflare.ini \
      -d ${domain} \
      --non-interactive \
      --agree-tos \
      -m ${letsencrypt_email}
  - systemctl reload nginx

  # Configure nginx
  - rm -f /etc/nginx/sites-enabled/default
  - ln -sf /etc/nginx/sites-available/ai /etc/nginx/sites-enabled/ai
  - systemctl enable nginx
  - systemctl start nginx

  # Install Promtail for log shipping
  - /opt/ai/install-promtail.sh

  # Enable and start setup service
  - systemctl daemon-reload
  - systemctl enable ai-setup.service
  - systemctl start ai-setup.service
